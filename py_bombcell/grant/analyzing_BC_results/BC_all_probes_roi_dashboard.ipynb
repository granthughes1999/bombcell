{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-probe Bombcell dashboard (ROI-aware)\n",
    "\n",
    "This notebook loads **all probes simultaneously** and builds ROI-aware tables/plots.\n",
    "\n",
    "## How to use `IN_ROI` / `OUTSIDE_ROI`\n",
    "1. JSON defaults are read from `probe_recording_roi` in your Grant config file.\n",
    "2. You can override any probe ROI in-notebook using `ROI_OVERRIDE_BY_PROBE`.\n",
    "3. Units with `distance_from_tip_um <= ROI_END_UM_BY_PROBE[probe]` are labeled `IN_ROI`.\n",
    "4. Units above that cutoff are labeled `OUTSIDE_ROI`.\n",
    "5. If a probe has no ROI value (`None`), units are labeled `ROI_NOT_SET`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import bombcell as bc\n",
    "\n",
    "analysis_dir = Path.cwd().resolve()\n",
    "sys.path.insert(0, str(analysis_dir))\n",
    "from post_analysis_setup import load_post_analysis_context, label_units_by_tip_distance\n",
    "\n",
    "CONFIG_FILE = '../configs/grant_recording_config.json'\n",
    "RUN_MODE = 'batch'  # batch | np20_rerun | single_probe\n",
    "\n",
    "# Optional notebook-only ROI overrides (um from tip)\n",
    "ROI_OVERRIDE_BY_PROBE = {\n",
    "    # 'B': 950,\n",
    "}\n",
    "TIP_POSITION_BY_PROBE = {\n",
    "    'A': 'min_y', 'B': 'min_y', 'C': 'min_y',\n",
    "    'D': 'min_y', 'E': 'min_y', 'F': 'min_y',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = load_post_analysis_context(CONFIG_FILE)\n",
    "mode_to_roots = {\n",
    "    'batch': ctx['DEFAULT_KS_STAGING_ROOT'],\n",
    "    'np20_rerun': ctx['NP20_KS_STAGING_ROOT'],\n",
    "    'single_probe': ctx['BOMBCELL_KS_SINGLEPROBE_STAGING_ROOT'],\n",
    "}\n",
    "staging_root = mode_to_roots[RUN_MODE]\n",
    "probe_letters = list(ctx['probeLetters'])\n",
    "\n",
    "# ROI defaults from JSON config, with notebook overrides layered on top\n",
    "ROI_END_UM_BY_PROBE = dict(ctx.get('PROBE_RECORDING_ROI', {}))\n",
    "ROI_END_UM_BY_PROBE.update(ROI_OVERRIDE_BY_PROBE)\n",
    "\n",
    "print('staging_root:', staging_root)\n",
    "print('probes:', probe_letters)\n",
    "print('ROI defaults/overrides:', ROI_END_UM_BY_PROBE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all probes and attach ROI labels + classification + reason flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_units = []\n",
    "missing_probes = []\n",
    "\n",
    "for probe in probe_letters:\n",
    "    ks_dir = Path(staging_root) / f'kilosort4_{probe}'\n",
    "    save_path = ks_dir / 'bombcell'\n",
    "    if not save_path.exists():\n",
    "        missing_probes.append(probe)\n",
    "        continue\n",
    "\n",
    "    param, quality_metrics, _ = bc.load_bc_results(str(save_path))\n",
    "    unit_type, unit_type_string = bc.qm.get_quality_unit_type(param, quality_metrics)\n",
    "\n",
    "    qm_df = pd.DataFrame(quality_metrics).copy()\n",
    "    qm_df['probe'] = probe\n",
    "    qm_df['ks_dir'] = str(ks_dir)\n",
    "    qm_df['save_path'] = str(save_path)\n",
    "    qm_df['bombcell_label'] = unit_type_string\n",
    "    qm_df['unit_index'] = np.arange(len(qm_df))\n",
    "\n",
    "    if 'cluster_id' not in qm_df.columns:\n",
    "        qm_df['cluster_id'] = qm_df['unit_index']\n",
    "\n",
    "    roi_end = ROI_END_UM_BY_PROBE.get(probe, None)\n",
    "    if roi_end is None:\n",
    "        qm_df['distance_from_tip_um'] = np.nan\n",
    "        qm_df['roi_label'] = 'ROI_NOT_SET'\n",
    "    else:\n",
    "        qm_df = label_units_by_tip_distance(\n",
    "            quality_metrics=qm_df,\n",
    "            ks_dir=ks_dir,\n",
    "            roi_end_um=float(roi_end),\n",
    "            tip_position=TIP_POSITION_BY_PROBE.get(probe, 'min_y'),\n",
    "            in_label='IN_ROI',\n",
    "            out_label='OUTSIDE_ROI',\n",
    "        )\n",
    "\n",
    "    # Build reason flags from Bombcell qm table (boolean columns)\n",
    "    qm_table = bc.make_qm_table(quality_metrics, param, unit_type_string)\n",
    "    bool_cols = [c for c in qm_table.columns if qm_table[c].dtype == bool]\n",
    "    reason_tokens = []\n",
    "    for _, row in qm_table[bool_cols].iterrows():\n",
    "        hits = [c for c in bool_cols if bool(row[c])]\n",
    "        reason_tokens.append(hits)\n",
    "\n",
    "    qm_df['reason_tokens'] = reason_tokens\n",
    "    qm_df['n_reason_flags'] = qm_df['reason_tokens'].apply(len)\n",
    "\n",
    "    all_units.append(qm_df)\n",
    "\n",
    "if len(all_units) == 0:\n",
    "    raise RuntimeError('No probe outputs were loaded. Check RUN_MODE/staging root and that bombcell outputs exist.')\n",
    "\n",
    "all_df = pd.concat(all_units, ignore_index=True)\n",
    "print('Loaded units:', len(all_df))\n",
    "print('Missing probes (no save_path):', missing_probes)\n",
    "print(all_df[['probe','bombcell_label','roi_label']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified all-probe dataframe views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_probe_roi_class = (\n",
    "    all_df.groupby(['probe', 'roi_label', 'bombcell_label'])\n",
    "    .size()\n",
    "    .rename('n_units')\n",
    "    .reset_index()\n",
    ")\n",
    "display(summary_probe_roi_class.sort_values(['probe', 'roi_label', 'bombcell_label']).head(200))\n",
    "\n",
    "pivot_probe_class = all_df.pivot_table(index='probe', columns='bombcell_label', values='cluster_id', aggfunc='count', fill_value=0)\n",
    "display(pivot_probe_class)\n",
    "\n",
    "pivot_probe_roi = all_df.pivot_table(index='probe', columns='roi_label', values='cluster_id', aggfunc='count', fill_value=0)\n",
    "display(pivot_probe_roi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI-aware plots across all probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1) Units per probe split by ROI label\n",
    "probe_roi = all_df.pivot_table(index='probe', columns='roi_label', values='cluster_id', aggfunc='count', fill_value=0)\n",
    "probe_roi.plot(kind='bar', stacked=True, ax=axes[0,0])\n",
    "axes[0,0].set_title('Units per probe (ROI split)')\n",
    "axes[0,0].set_ylabel('n units')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2) Units per class split by ROI label\n",
    "class_roi = all_df.pivot_table(index='bombcell_label', columns='roi_label', values='cluster_id', aggfunc='count', fill_value=0)\n",
    "class_roi.plot(kind='bar', stacked=True, ax=axes[0,1])\n",
    "axes[0,1].set_title('Bombcell classes (ROI split)')\n",
    "axes[0,1].set_ylabel('n units')\n",
    "axes[0,1].tick_params(axis='x', rotation=35)\n",
    "\n",
    "# 3) Mean reason flags per class and ROI\n",
    "mean_flags = all_df.groupby(['bombcell_label','roi_label'])['n_reason_flags'].mean().unstack(fill_value=0)\n",
    "mean_flags.plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Mean # reason flags per unit (class \u00d7 ROI)')\n",
    "axes[1,0].set_ylabel('mean n_reason_flags')\n",
    "axes[1,0].tick_params(axis='x', rotation=35)\n",
    "\n",
    "# 4) Probe \u00d7 class heatmap for IN_ROI only (or fallback ALL)\n",
    "in_roi = all_df[all_df['roi_label'] == 'IN_ROI']\n",
    "plot_df = in_roi if len(in_roi) > 0 else all_df\n",
    "heat = plot_df.pivot_table(index='probe', columns='bombcell_label', values='cluster_id', aggfunc='count', fill_value=0)\n",
    "im = axes[1,1].imshow(heat.to_numpy(), aspect='auto')\n",
    "axes[1,1].set_xticks(np.arange(len(heat.columns)))\n",
    "axes[1,1].set_xticklabels(list(heat.columns), rotation=35, ha='right')\n",
    "axes[1,1].set_yticks(np.arange(len(heat.index)))\n",
    "axes[1,1].set_yticklabels(list(heat.index))\n",
    "axes[1,1].set_title('Probe \u00d7 class heatmap (' + ('IN_ROI' if len(in_roi)>0 else 'ALL units') + ')')\n",
    "plt.colorbar(im, ax=axes[1,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason-level plots with ROI as a key dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# explode reasons for long-form reason table\n",
    "reason_rows = []\n",
    "for _, row in all_df.iterrows():\n",
    "    for r in row['reason_tokens']:\n",
    "        reason_rows.append({'probe': row['probe'], 'roi_label': row['roi_label'], 'bombcell_label': row['bombcell_label'], 'reason': r})\n",
    "reason_df = pd.DataFrame(reason_rows)\n",
    "\n",
    "if len(reason_df) == 0:\n",
    "    print('No reason flags available in reason_df')\n",
    "else:\n",
    "    # top reasons overall\n",
    "    top_reasons = reason_df['reason'].value_counts().head(12).index\n",
    "    rr = reason_df[reason_df['reason'].isin(top_reasons)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(23, 6))\n",
    "\n",
    "    # A) top reasons by ROI\n",
    "    a = rr.groupby(['reason','roi_label']).size().unstack(fill_value=0).loc[top_reasons]\n",
    "    a.plot(kind='barh', stacked=True, ax=axes[0])\n",
    "    axes[0].set_title('Top reason counts (ROI split)')\n",
    "    axes[0].set_xlabel('count')\n",
    "\n",
    "    # B) top reasons by probe (stacked)\n",
    "    b = rr.groupby(['reason','probe']).size().unstack(fill_value=0).loc[top_reasons]\n",
    "    b.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "    axes[1].set_title('Top reason counts by probe')\n",
    "    axes[1].tick_params(axis='x', rotation=60)\n",
    "\n",
    "    # C) reason heatmap for IN_ROI (or OUTSIDE fallback)\n",
    "    focus = rr[rr['roi_label']=='IN_ROI']\n",
    "    if len(focus)==0:\n",
    "        focus = rr[rr['roi_label']=='OUTSIDE_ROI']\n",
    "    h = focus.pivot_table(index='reason', columns='probe', values='bombcell_label', aggfunc='count', fill_value=0)\n",
    "    h = h.loc[h.sum(axis=1).sort_values(ascending=False).head(12).index]\n",
    "    im = axes[2].imshow(h.to_numpy(), aspect='auto')\n",
    "    axes[2].set_yticks(np.arange(len(h.index)))\n",
    "    axes[2].set_yticklabels(list(h.index))\n",
    "    axes[2].set_xticks(np.arange(len(h.columns)))\n",
    "    axes[2].set_xticklabels(list(h.columns))\n",
    "    axes[2].set_title('Reason heatmap in ROI focus set')\n",
    "    plt.colorbar(im, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual-unit + probe-level drill-down (ROI-aware)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick any probe and class; then inspect units + reason flags\n",
    "DRILL_PROBE = 'B'\n",
    "DRILL_CLASS = None  # e.g. 'MUA', 'GOOD', 'NOISE', 'NON-SOMA'; None keeps all\n",
    "DRILL_ROI = 'IN_ROI'  # IN_ROI | OUTSIDE_ROI | ROI_NOT_SET | None\n",
    "\n",
    "sub = all_df[all_df['probe'] == DRILL_PROBE].copy()\n",
    "if DRILL_CLASS is not None:\n",
    "    sub = sub[sub['bombcell_label'].astype(str).str.contains(DRILL_CLASS, regex=False)]\n",
    "if DRILL_ROI is not None:\n",
    "    sub = sub[sub['roi_label'] == DRILL_ROI]\n",
    "\n",
    "print('n units in drill subset:', len(sub))\n",
    "display(sub[['probe','cluster_id','bombcell_label','roi_label','distance_from_tip_um','n_reason_flags']].head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sub) == 0:\n",
    "    print('Drill subset empty. Adjust DRILL_* filters.')\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # distance distribution by class within drill subset\n",
    "    if sub['distance_from_tip_um'].notna().any():\n",
    "        for label, grp in sub.groupby('bombcell_label'):\n",
    "            vals = grp['distance_from_tip_um'].dropna().to_numpy()\n",
    "            if len(vals) > 0:\n",
    "                axes[0].hist(vals, bins=30, alpha=0.5, label=str(label))\n",
    "        axes[0].set_title(f'Distance-from-tip distribution ({DRILL_PROBE}, ROI={DRILL_ROI})')\n",
    "        axes[0].set_xlabel('distance_from_tip_um')\n",
    "        axes[0].set_ylabel('n units')\n",
    "        axes[0].legend(fontsize=8)\n",
    "    else:\n",
    "        axes[0].text(0.5,0.5,'No distance data for this subset',ha='center',va='center')\n",
    "\n",
    "    # top reasons in drill subset\n",
    "    from collections import Counter\n",
    "    c = Counter(r for row in sub['reason_tokens'] for r in row)\n",
    "    if c:\n",
    "        top = c.most_common(15)\n",
    "        labs = [k for k,_ in top][::-1]\n",
    "        vals = [v for _,v in top][::-1]\n",
    "        axes[1].barh(labs, vals)\n",
    "    else:\n",
    "        axes[1].text(0.5,0.5,'No reason flags in subset',ha='center',va='center')\n",
    "    axes[1].set_title(f'Top reasons ({DRILL_PROBE}, ROI={DRILL_ROI})')\n",
    "    axes[1].set_xlabel('count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = Path(staging_root) / f'all_probes_{RUN_MODE}_roi_audit.csv'\n",
    "all_df.to_csv(out_csv, index=False)\n",
    "print('Saved all-probe ROI-aware dataframe to:', out_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
